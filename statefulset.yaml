#
#  Author: Hari Sekhon
#  Date: 2019-11-28 18:51:59 +0000 (Thu, 28 Nov 2019)
#
#  vim:ts=2:sts=2:sw=2:et
#
#  https://github.com/harisekhon/kubernetes-templates
#
#  License: see accompanying Hari Sekhon LICENSE file
#
#  If you're using my code you're welcome to connect with me on LinkedIn
#  and optionally send me feedback to help improve or steer this or other code I publish
#
#  https://www.linkedin.com/in/HariSekhon
#

# ============================================================================ #
#                             S t a t e f u l S e t
# ============================================================================ #

# https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
#
# see pod anti-affinity here:
#
# https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: NAME
  namespace: NAMESPACE
  annotations:
    # see also k8s_pod_disruption_budget.yaml
    cluster-autoscaler.kubernetes.io/safe-to-evict: False
spec:
  replicas: 3
  serviceName: NAME
  selector:
    matchLabels:
      app: NAME
  nodeSelector:
    disktype: ssd
  template:
    metadata:
      labels:
        app: NAME
    spec:
      priorityClassName: high-priority  # to get priority access to the stable node pool instead of running on preemptible node pool risking pv corruption, will evict lower priority pods to preemptible node pool (default 0) if necessary
      affinity:
        nodeAffinity:
          # Node preemption:
          # - shuts down pods ungracefully
          # - ignores PodDisruptionBudget
          # - invalidate Stateful guarantees and can lead to data loss !!
          # https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#kubernetes_constraint_violations
          # https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices
#          requiredDuringSchedulingIgnoredDuringExecution:
#            nodeSelectorTerms:
#              - matchExpressions:
#                - key: cloud.google.com/gke-preemptible
#                  operator: DoesNotExist
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: cloud.google.com/gke-preemptible
                    operator: DoesNotExist
      # tip: taint the non-preemptible nodes eg. kubectl taint nodes node1 "cloud.google.com/gke-preemptible"="false":PreferNoSchedule
      #      then add tolerance so only select deployments get priority scheduling on stable non-preemptible nodes
      tolerations:
        - key: cloud.google.com/gke-preemptible
          operator: Equal
          value: "false"
          effect: PreferNoSchedule
      #nodeSelector:
      #  # GKE specific - hard requirement - preferred affinity above is safer soft requirement to still allow scheduling if node pool becomes unavailable (eg. recreated by Terraform)
      #  cloud.google.com/gke-nodepool: POOL_NAME
      containers:
      - name: NAME
        image: postgres:9.6
        ports:
          - name: NAME
            containerPort: 5432
        env:
          - name: HEAP_SIZE
            value: 1G
          - name: CASSANDRA_SEEDS
            value: cassandra-0.cassandra.svc.cluster.local,cassandra-1.cassandra.svc.cluster.local,cassandra-2.cassandra.svc.cluster.local
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP  # K8S API path
          - name: MYSECRET
            valueFrom:
              secretKeyRef:
                name: my-secret
                key: my-secret
        readinessProbe:
          # http, tcp or exec
          #httpGet:
          #  path: /healthz
          #  port: 8080
          # only uncomment if you can't use http check, must comment/delete http check otherwise will get this error:
          # invalid: spec.containers[0].livenessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type
          tcpSocket:
            port: 5432
          initialDelaySeconds: 0 # default
          successThreshold: 1    # default
          failureThreshold: 3    # default
          periodSeconds: 10      # default (interval)
          timeoutSeconds: 1      # default
        livenessProbe:
          #httpGet:
          #  path: /healthz
          #  port: 8080
          tcpSocket:
            port: 5432
          initialDelaySeconds: 0 # default
          successThreshold: 1    # default
          failureThreshold: 3    # default
          periodSeconds: 10      # default (interval)
          timeoutSeconds: 1      # default
        volumeMounts:
          - name: pgdata
            mountPath: /var/lib/pgsql
  volumeClaimTemplates:
  - metadata:
      name: pgdata
    spec:
      # optional: requires storageclass-*.yaml
      #storageClassName: ssd
      #storageClassName: ssd-resizeable
      #storageClassName: standard-resizeable
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          # resize here doesn't affect the PVC, must edit the PVC directly to trigger the provider resize, didn't even need a pod cycle in GCP testing
          storage: 10Gi
